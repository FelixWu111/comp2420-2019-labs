{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'> COMP2420/COMP6420 - Introduction to Data Management, Analysis and Security</h1>\n",
    "\n",
    "<h2 align='center'> Lab 06 - Machine Learning - II</h2>\n",
    "\n",
    "*****\n",
    "\n",
    "In this lab, we will first guide you through some required interfaces of `sklearn`. We will implement and study **Decision Trees** and **Nearest-Neighbours Regression** implimented using `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important Imports\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "A **Decision Tree** is a **supervised** learning algorithm used for classification problems. In this algorithm, we split the original data into two or more homogeneous sets. This is done based on most significant attributes to make the resulting groups as distinct as possible. In decision analysis, a decision tree is used to visually and explicitly represent decisions and decision making. It uses a tree-like model of decisions. A decision tree is drawn with its **root** at the top and **branches** at the bottom. The branch end that doesn’t split anymore is the **decision / leaf**.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1. Pick an attribute to split at a non-terminal node (based on which attribute will provide the greatest **Information Gain**).\n",
    "2. Split examples into groups based on attribute value.\n",
    "3. For each group:\n",
    "    * **If** no examples – return majority from parent\n",
    "    * **Else If** all examples in same class – return class\n",
    "    * **Else** loop to Step 1\n",
    "\n",
    "\n",
    "### Properties\n",
    "\n",
    "* Internal nodes test attributes.\n",
    "* Branching is determined by attribute value.\n",
    "* Leaf nodes are outputs (class assignments).\n",
    "\n",
    "\n",
    "\n",
    "Let's try to make some predictions with such a decision tree!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Sink or Float\n",
    "\n",
    "We will use the **Titanic Passenger Dataset** which has data related to 891 passengers and 11 features + the target variable (Survived) in the dataset. The dataset is provided under the `data` folder and has the following features:\n",
    "\n",
    "| **Feature**              |**Description**                                                                   |\n",
    "|--------------------------|----------------------------------------------------------------------------------|\n",
    "| PassengerId              | ID of Passenger in dataset                                                       |\n",
    "| Survived                 | Whether the passenger survived or not {0: No, 1: Yes}                            |\n",
    "| Pclass                   | Class of Travel (1,2 or 3)                                                       |\n",
    "| Name                     | Name of Passenger                                                                |\n",
    "| Sex                      | Gender of Passenger (Male or Female)                                             |\n",
    "| Age                      | Age of Passenger                                                                 |\n",
    "| Sibsp                    | Number of Siblings/Spouse aboard                                                 |\n",
    "| Parch                    | Number of Parent/Child aboard                                                    |\n",
    "| Ticket                   | Ticket Number                                                                    |\n",
    "| Fare                     | Cost of the Ticket                                                               |\n",
    "| Cabin                    | Cabin number of Passenger                                                        |\n",
    "| Embarked                 | Port which the Passenger embarked {C: Cherbourg, S: Southhampton, Q: Queenstown} |\n",
    "\n",
    "\n",
    "We are going to split this dataset into **train** and **test** data. We aim to build a **decision tree to predict whether a passenger will survive or not in the titanic crash**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Complete the following steps to prepare the data for use:\n",
    "    - Load the dataset `titanic.csv` into a dataframe. Do a quick check on the type of data each column has. \n",
    "    - To make life easier for the decision tree, alter the **Sex** column such that `male` is replaced with `0` & `female` is replaced with `1`\n",
    "    - Clean up the **Age** column to remove any `NaN` values (**HINT**: Replace them with the mean of the entire column).\n",
    "    - For this exercise we would need only need the feature columns `Pclass`, `Sex`, `Age`, `SibSp` and `Parch`, and the result `Survived`. Split the DataFrame into two new variables, one being a DataFrame with the feature columns and the other holding the result data\n",
    "    - Using `sklearn`'s train-test-split function, split the data into a training set and testing set to evaluate your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(623, 5) (268, 5) \n",
      " (623, 1) (268, 1)\n"
     ]
    }
   ],
   "source": [
    "# YOUR ANSWER HERE\n",
    "data = pd.read_csv('data/titanic.csv')\n",
    "data['Sex'].replace(['female','male'],[0,1], inplace=True)\n",
    "data['Age'] = data['Age'].fillna(data['Age'].mean())\n",
    "feature = data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch']]\n",
    "result = data[['Survived']]\n",
    "x_train, x_test, y_train, y_test = train_test_split(feature, result, test_size = 0.3, random_state = 42)\n",
    "print(x_train.shape, x_test.shape, '\\n', y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can you do with a decision tree object?\n",
    "\n",
    "| Object Method | Description |\n",
    "| --- |:---:|\n",
    "| `dt.apply()` | **Returns the index of the leaf that each sample is predicted as.** |\n",
    "| `dt.decision_path()` | **Return the decision path in the tree** |\n",
    "| `dt.fit()` | **Build a decision tree classifier from the training set.** |\n",
    "| `dt.predict()` | **Predict class value for X.** |\n",
    "| `dt.score()` | **Returns the mean accuracy on the given test data and labels.** |\n",
    "\n",
    "<br/>\n",
    "\n",
    "2. Using the `sklearn` module, make a decision tree object and fit it to the `training` dataset. Determine the accuracy of this model on the `test` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.746268656716418"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR ANSWER HERE\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(x_train, y_train)\n",
    "y_pred = dt.predict(x_test)\n",
    "dt.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How would the accuracy be affected if you increase or decrease the `DecisionTreeClassifier` object parameters like `max_depth` and `min_samples_leaf`? You are welcome to try making the `DecisionTreeClassifier` object with different values of `max_depth` and `min_samples_leaf` and compare your model's accuracy. But, make sure you can justify the change in accuracy for the increase or decrease in these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7985074626865671 \n",
      " 0.8171641791044776\n"
     ]
    }
   ],
   "source": [
    "# YOUR ANSWER HERE\n",
    "dt_1 = DecisionTreeClassifier(max_depth = 3, min_samples_leaf = 5)\n",
    "dt_1.fit(x_train, y_train)\n",
    "dt_2 = DecisionTreeClassifier(max_depth = 4, min_samples_leaf = 7)\n",
    "dt_2.fit(x_train, y_train)\n",
    "print(dt_1.score(x_test, y_test), '\\n',dt_2.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Justification\n",
    "- As necessary. Use this to put down notes that you can come back to quickly later when you're studying !\n",
    "- prediction accuracy will somehow increase with the increasement of max_depth and min_samples_leave and remain stable after a upper bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Scikit-Learn provides you with an easy way to visualize and export this **Decision Tree**. Export this decision tree to your machine and visually inspect it (**HINT**: This was performed in the lectures)\n",
    "\n",
    "Note: If you're doing this at home, you may require extra files to compile the dot file to visually inspect. The steps are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE\n",
    "dotf = export_graphviz(dt_2, out_file='out.dot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling the `.dot` file (using Graphviz)\n",
    "\n",
    "##### OSX (Using Homebrew)\n",
    "- In an open terminal window:\n",
    "    - `brew install graphviz`\n",
    "    - `dot -Tpng decision_tree.dot -o decision_tree.png` (when in the directory of the `.dot` file)\n",
    "\n",
    "##### Ubuntu \n",
    "- In an open terminal window\n",
    "    - `sudo apt install graphviz` (if not already installed, can sometimes come standard) **(On CECS computers, it will be automatically installed)**\n",
    "    - `dot -Tpng decision_tree.dot -o decision_tree.png` (when in the directory of the `.dot` file)\n",
    "    \n",
    "##### Windows\n",
    "- Install the appropiate packages from (here)[https://graphviz.gitlab.io/_pages/Download/Download_windows.html]\n",
    "- Ensure Graphiz is within your PATH\n",
    "- Using Powershell\n",
    "    - `dot -Tpng decision_tree.dot -o decision_tree.png` (when in the directory of the `.dot` file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## k-Nearest Neighbours\n",
    "\n",
    "A **k-Nearest Neighbours** is a **supervised learning** algorithm. It is extremely easy to implement in its most basic form, and yet performs quite complex classification tasks. It is a lazy learning algorithm since it doesn't have a specialized training phase. Rather, it uses all of the data for training while classifying a new data point or instance. \n",
    "\n",
    "KNN is a **non-parametric learning algorithm**, which means that it doesn't assume anything about the underlying data. This is an extremely useful feature since most of the real world data doesn't really follow any theoretical assumption e.g. linear-separability, uniform distribution, etc.\n",
    "\n",
    "<img src='./images/knn_classification.jpg'>\n",
    "\n",
    "Source: [E_blog - K-Nearest Neighbour(KNN) Classification](https://zeidigital.wordpress.com/2016/08/13/k-nearest-neighbour-classification-algorithm-implementation-in-python/)\n",
    "\n",
    "Given a dataset with **x and y**, Nearest Neighbours Regression can be used to:\n",
    "\n",
    "* Build a predictive model to predict future values of **x<sub>i</sub>** without a **y<sub>i</sub>** value.\n",
    "* Build a model without assuming any parameters, **K** is hyperparameter.\n",
    "* It can be seen as a baseline method to compare your complex models.\n",
    "\n",
    "Lets try doing some predictions using a k-Nearest Neighbours algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Want to buy\n",
    "\n",
    "We have provided a dataset of 200 consumers and 3 features and a target variable in the dataset. The dataset is provided under the `data` folder and has the following features:\n",
    "\n",
    "| **Feature**              |**Description**                                                                   |\n",
    "|--------------------------|----------------------------------------------------------------------------------|\n",
    "| CustomerID               | Unique ID assigned to the customer                                               |\n",
    "| Gender                   | Gender of the Shopper (Male or Female)                                           |\n",
    "| Age                      | Age of the Shopper                                                               |\n",
    "| Annual Income            | Annual Income of the customer (in thousands)                                     |\n",
    "| Will Buy                 | Whether the customer will buy item x {Yes: 1, No: 0}                             |\n",
    "\n",
    "#### Scenario\n",
    "You run a local grocery store and have been approached regarding stocking a products. Product-x is being marketed as the biggest thing since sliced bread in the next town over, although you're unsure if your customers will be as responsive. You decide to purchase a test batch and allow your 200 loyalty members to review the product, and use their buying statistics to determine whether you should stock the product permanently. Loyalty memebers will inform you whether they brought the product or not, and you can use this to predict whether other customers will do the same.\n",
    "\n",
    "Note: This is another example of a binary classification problem, as you are trying to determine whether the answer will be \"yes\" or \"no\" given a question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Complete the following steps to prepare the data for use:\n",
    "    - Load the dataset `buying_cx.csv` into a dataframe, using CustomerID as the index column\n",
    "    - As with the previous task, alter the `Gender` column to reflect 0 meaning Male & 1 meaning Female (impliment this using a different method to the previous question) \n",
    "        - **Extension**: Use a LabelEncoder or the like from the `sklearn` module to change the values.\n",
    "    - Split the data into a training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Female' 'Male']\n",
      "   CustomerID  Gender  Age  Annual Income (k$)  Will Buy\n",
      "0           1       1   19                  15         0\n",
      "1           2       1   21                  15         1\n",
      "2           3       0   20                  16         0\n",
      "3           4       0   23                  16         1\n",
      "4           5       0   31                  17         0\n",
      "train: (160, 3)   (40, 3) \n",
      " (160,)   (40,)\n"
     ]
    }
   ],
   "source": [
    "# YOUR ANSWER HERE\n",
    "data2 = pd.read_csv('data/buying_cx.csv')\n",
    "# create an LabelEncoder object\n",
    "le = LabelEncoder()\n",
    "# fit the pandas column into the LabelEncoder\n",
    "le.fit(data2.Gender)\n",
    "# view the fitted classed\n",
    "print(le.classes_)\n",
    "# apply the fitted LabelEncoder to the pandas column\n",
    "data2.Gender = le.transform(data2.Gender)\n",
    "# view the transformed dataFrame\n",
    "print(data2.head())\n",
    "# split the data into a training and testing dataset\n",
    "feature2 = data2[['Gender', 'Age', 'Annual Income (k$)']]\n",
    "result2 = data2['Will Buy']\n",
    "x2_train,  x2_test, y2_train, y2_test = train_test_split(feature2, result2, test_size = 0.2, random_state = 42)\n",
    "# View the shapes to ensure we do it right\n",
    "print('train:', x2_train.shape, ' ', x2_test.shape, '\\n', y2_train.shape, ' ', y2_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can you do with a k-Nearest Neighbors object?\n",
    "While there are other methods, the main functions you will require for this exercise are the following:\n",
    "\n",
    "| Method | Description |\n",
    "| --- |:---:|\n",
    "| `.fit()` | **Build a decision tree classifier from the training set.** |\n",
    "| `.predict()` | **Predict class value for X.** |\n",
    "| `.score()` | **Returns the mean accuracy on the given test data and labels.** |\n",
    "\n",
    "<br/>\n",
    "\n",
    "2. Using the k-Nearest Neighbors class from `sklearn`, impliment a k-Nearest Neighbors classifier where it checks the 5 closest neighbours, and determine the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40,)\n",
      "0.725\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "# Fit the train data into the knn model\n",
    "knn.fit(x2_train, y2_train)\n",
    "# predict the result for test data\n",
    "Ypred2 = knn.predict(x2_test)\n",
    "# View the predicted shape of result to ensure we stay on the right track\n",
    "print(Ypred2.shape)\n",
    "# determine the accuracy of the model\n",
    "print(knn.score(x2_test, y2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. As k is a hyperparameter that we specify when we create the model, it is possible to adjust the number of neighbours the model will check for a solution. Create 3 more models, each with a different k value (k=1, k=50, k=150) and compare the scores of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65 0.575 0.7\n"
     ]
    }
   ],
   "source": [
    "# Create three models for k=1, k=50, k=150\n",
    "knn_1 = KNeighborsClassifier(n_neighbors=1)\n",
    "knn_2 = KNeighborsClassifier(n_neighbors=50)\n",
    "knn_3 = KNeighborsClassifier(n_neighbors=150)\n",
    "# Fit the train data into three knn models\n",
    "knn_1.fit(x2_train, y2_train)\n",
    "knn_2.fit(x2_train, y2_train)\n",
    "knn_3.fit(x2_train, y2_train)\n",
    "# print the scores for each model\n",
    "print(knn_1.score(x2_test, y2_test), knn_2.score(x2_test, y2_test), knn_3.score(x2_test, y2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing these scores to when we tried for the 5 closest neighbours, what does this tell us about the optimal number of neighbours? Will the optimal number of neighbours remain constant over different divisions of testing and training data? Discuss this with other members of the course or your tutor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTES\n",
    "- Insert Notes from your discussion here as necessary\n",
    "Self-thoughts:\n",
    "The optimal number will remain constant over different divisions of testing and training data. When more data is used for training purpose, the prediction accuracy will be higher but the 5-sample one is still the highest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
